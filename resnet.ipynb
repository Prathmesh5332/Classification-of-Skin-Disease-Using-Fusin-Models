{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hm6BuazOG1QJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import DenseNet201, ResNet152V2, VGG16\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Concatenate, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "import os\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the dataset is extracted from Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuUGJQCcJyUm",
        "outputId": "a2a850bd-88f9-4e2a-c93c-7a87f88109f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths to dataset\n",
        "zip_path = '/content/drive/MyDrive/Dataset/SkinDiseases.zip'\n",
        "extract_path = '/content/skin_disease_dataset'\n",
        "\n",
        "# Extract the dataset\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print('Dataset extracted successfully.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMXK2so3KFWV",
        "outputId": "411d6c1b-c019-4363-f8ef-6690b082fc65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset extracted successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir_train = os.path.join(extract_path, 'SkinDisease/train')\n",
        "data_dir_test = os.path.join(extract_path, 'SkinDisease/test')"
      ],
      "metadata": {
        "id": "uFbDIXvNKdEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing and Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2  # Split for training and validation\n",
        ")\n"
      ],
      "metadata": {
        "id": "1joVvU5zK9pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "    data_dir_train,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2HBf9pQK_ed",
        "outputId": "46b0a2ad-9e3f-4fd2-f2b8-11984b49b34b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 11128 images belonging to 22 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    data_dir_train,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff3jAtu-LC37",
        "outputId": "3cdce8f8-b144-47ce-ec86-06201e56f06d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2770 images belonging to 22 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fusion Model Development\n",
        "input_shape = (256, 256, 3)\n",
        "input_tensor = Input(shape=input_shape, name=\"input_image\")"
      ],
      "metadata": {
        "id": "-JVunjHyLIG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DenseNet201 Model\n",
        "input_tensor_densenet = Input(shape=input_shape)\n",
        "base_model_densenet = DenseNet201(weights='imagenet', include_top=False, input_tensor=input_tensor_densenet)\n",
        "base_model_densenet.trainable = False  # Freeze layers\n",
        "x_densenet = base_model_densenet.output\n",
        "x_densenet = GlobalAveragePooling2D()(x_densenet)\n",
        "x_densenet = Dense(512, activation='relu')(x_densenet)\n",
        "output_densenet = Dense(train_generator.num_classes, activation='softmax')(x_densenet)\n",
        "\n",
        "model_densenet = Model(inputs=input_tensor_densenet, outputs=output_densenet)\n",
        "\n",
        "# Compile the model\n",
        "model_densenet.compile(optimizer=Adam(),\n",
        "                       loss='categorical_crossentropy',\n",
        "                       metrics=['accuracy'])\n",
        "# Callbacks\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.75, patience=5, min_lr=1e-5, verbose=1)\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "history_densenet = model_densenet.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=30,\n",
        "    callbacks=[reduce_lr]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGIFbQIFLKTa",
        "outputId": "cd5b94e1-cd54-4437-ae58-6c31f1a4e588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 920ms/step - accuracy: 0.3292 - loss: 2.3012 - val_accuracy: 0.3513 - val_loss: 2.1501 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 830ms/step - accuracy: 0.4892 - loss: 1.6516 - val_accuracy: 0.3708 - val_loss: 2.1420 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 763ms/step - accuracy: 0.5451 - loss: 1.4670 - val_accuracy: 0.3606 - val_loss: 2.2386 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 755ms/step - accuracy: 0.5832 - loss: 1.3405 - val_accuracy: 0.3787 - val_loss: 2.1764 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 755ms/step - accuracy: 0.6193 - loss: 1.2223 - val_accuracy: 0.3866 - val_loss: 2.2048 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 764ms/step - accuracy: 0.6309 - loss: 1.1690 - val_accuracy: 0.3866 - val_loss: 2.2572 - learning_rate: 0.0010\n",
            "Epoch 7/30\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 594ms/step - accuracy: 0.6612 - loss: 1.0502\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0007500000356230885.\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 768ms/step - accuracy: 0.6612 - loss: 1.0503 - val_accuracy: 0.3621 - val_loss: 2.3196 - learning_rate: 0.0010\n",
            "Epoch 8/30\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 759ms/step - accuracy: 0.6980 - loss: 0.9636 - val_accuracy: 0.3791 - val_loss: 2.3234 - learning_rate: 7.5000e-04\n",
            "Epoch 9/30\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 753ms/step - accuracy: 0.7262 - loss: 0.8672 - val_accuracy: 0.3848 - val_loss: 2.3582 - learning_rate: 7.5000e-04\n",
            "Epoch 10/30\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 753ms/step - accuracy: 0.7470 - loss: 0.8033 - val_accuracy: 0.3823 - val_loss: 2.4416 - learning_rate: 7.5000e-04\n",
            "Epoch 11/30\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 748ms/step - accuracy: 0.7658 - loss: 0.7577 - val_accuracy: 0.3986 - val_loss: 2.4018 - learning_rate: 7.5000e-04\n",
            "Epoch 12/30\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 594ms/step - accuracy: 0.7749 - loss: 0.7168\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0005625000048894435.\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 756ms/step - accuracy: 0.7749 - loss: 0.7168 - val_accuracy: 0.3838 - val_loss: 2.4201 - learning_rate: 7.5000e-04\n",
            "Epoch 13/30\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 821ms/step - accuracy: 0.8048 - loss: 0.6441 - val_accuracy: 0.3931 - val_loss: 2.4405 - learning_rate: 5.6250e-04\n",
            "Epoch 14/30\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 749ms/step - accuracy: 0.8125 - loss: 0.6042 - val_accuracy: 0.3903 - val_loss: 2.5347 - learning_rate: 5.6250e-04\n",
            "Epoch 15/30\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 752ms/step - accuracy: 0.8240 - loss: 0.5697 - val_accuracy: 0.4083 - val_loss: 2.4420 - learning_rate: 5.6250e-04\n",
            "Epoch 16/30\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 747ms/step - accuracy: 0.8352 - loss: 0.5445 - val_accuracy: 0.3910 - val_loss: 2.6129 - learning_rate: 5.6250e-04\n",
            "Epoch 17/30\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 590ms/step - accuracy: 0.8472 - loss: 0.4949\n",
            "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0004218749818392098.\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 752ms/step - accuracy: 0.8471 - loss: 0.4950 - val_accuracy: 0.4004 - val_loss: 2.5839 - learning_rate: 5.6250e-04\n",
            "Epoch 18/30\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 825ms/step - accuracy: 0.8633 - loss: 0.4552 - val_accuracy: 0.3913 - val_loss: 2.6437 - learning_rate: 4.2187e-04\n",
            "Epoch 19/30\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 752ms/step - accuracy: 0.8791 - loss: 0.4180 - val_accuracy: 0.3888 - val_loss: 2.6383 - learning_rate: 4.2187e-04\n",
            "Epoch 20/30\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 754ms/step - accuracy: 0.8769 - loss: 0.4012 - val_accuracy: 0.3993 - val_loss: 2.6733 - learning_rate: 4.2187e-04\n",
            "Epoch 21/30\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 750ms/step - accuracy: 0.8863 - loss: 0.3876 - val_accuracy: 0.4040 - val_loss: 2.7071 - learning_rate: 4.2187e-04\n",
            "Epoch 22/30\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 588ms/step - accuracy: 0.8893 - loss: 0.3719\n",
            "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.00031640623637940735.\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 750ms/step - accuracy: 0.8893 - loss: 0.3719 - val_accuracy: 0.3989 - val_loss: 2.7405 - learning_rate: 4.2187e-04\n",
            "Epoch 23/30\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 753ms/step - accuracy: 0.8968 - loss: 0.3500 - val_accuracy: 0.3982 - val_loss: 2.7294 - learning_rate: 3.1641e-04\n",
            "Epoch 24/30\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 755ms/step - accuracy: 0.9054 - loss: 0.3364 - val_accuracy: 0.4058 - val_loss: 2.6983 - learning_rate: 3.1641e-04\n",
            "Epoch 25/30\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 751ms/step - accuracy: 0.9034 - loss: 0.3305 - val_accuracy: 0.4058 - val_loss: 2.7159 - learning_rate: 3.1641e-04\n",
            "Epoch 26/30\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 747ms/step - accuracy: 0.9111 - loss: 0.3046 - val_accuracy: 0.4101 - val_loss: 2.7305 - learning_rate: 3.1641e-04\n",
            "Epoch 27/30\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 586ms/step - accuracy: 0.9182 - loss: 0.3015\n",
            "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.00023730468819849193.\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 747ms/step - accuracy: 0.9182 - loss: 0.3015 - val_accuracy: 0.4076 - val_loss: 2.8016 - learning_rate: 3.1641e-04\n",
            "Epoch 28/30\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 750ms/step - accuracy: 0.9186 - loss: 0.2850 - val_accuracy: 0.3949 - val_loss: 2.8460 - learning_rate: 2.3730e-04\n",
            "Epoch 29/30\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 754ms/step - accuracy: 0.9311 - loss: 0.2583 - val_accuracy: 0.4097 - val_loss: 2.8324 - learning_rate: 2.3730e-04\n",
            "Epoch 30/30\n",
            "\u001b[1m348/348\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 755ms/step - accuracy: 0.9278 - loss: 0.2582 - val_accuracy: 0.4007 - val_loss: 2.8513 - learning_rate: 2.3730e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_densenet.save('/content/drive/MyDrive/my_model.h5')"
      ],
      "metadata": {
        "id": "6naPHRZGzrpW",
        "outputId": "2b9e3487-e1ee-462a-d6db-d782deed8686",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    }
  ]
}